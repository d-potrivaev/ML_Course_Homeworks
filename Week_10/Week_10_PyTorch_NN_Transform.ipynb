{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67af5b03-3004-4cf8-8ae2-8c94ae79b351",
   "metadata": {},
   "source": [
    "# NLP From Scratch: Translation with a Sequence to Sequence Network and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0aa31-664b-4948-9159-fc8220f9d5ee",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0002e6f-bd0e-4e3c-a4bc-95e2990e98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103306d3-028b-47a0-85d7-674bc3d45e1e",
   "metadata": {},
   "source": [
    "### Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3928d6a-4291-491f-9f86-8d74d7d45715",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375b3d58-c14c-4fe4-a44c-19ecebdfb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper class\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # word to index dictionary\n",
    "        self.word2index = {}\n",
    "        # count of each word\n",
    "        self.word2count = {}\n",
    "        # index to word dictionary\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd0312e-11fb-4f14-ad3e-0ae6f6510c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f69d2252-5884-4dce-93cb-acf6a20e2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6494a2b9-516b-41f3-908f-2846993ffc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/Personal_CG_Learning/REBELWAY/Intro_to_Machine_Learning/Source/Datasets/NLP_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456d1dd4-f133-4657-a0f3-e47a5156a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file is English to French, but in the case of translate from French to English\n",
    "# exists the reverse flag to reverse the pairs\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(data_path + '/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2483baa3-1591-4818-b113-49bf2f4f720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train quickly, we’ll trim the data set to only relatively short and simple sentences with \n",
    "# maximum length is 10 words (that includes ending punctuation)\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "# And filtering to sentences that translate to the form “I am” or “He is” etc.\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p, r):\n",
    "    if r:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
    "    else:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs, r):\n",
    "    return [pair for pair in pairs if filterPair(pair, r)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2b18a6-2e60-43d9-ac77-e37deeb80f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 13056 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3378\n",
      "fra 5161\n",
      "['i am dying for a cold drink', 'je creve d envie d une boisson fraiche']\n"
     ]
    }
   ],
   "source": [
    "# The full process for preparing the data is:\n",
    "\n",
    "# - Read text file and split into lines, split lines into pairs\n",
    "# - Normalize text, filter by length and content\n",
    "# - Make word lists from sentences in pairs\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs, reverse)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10051e34-8bbb-4992-9ffe-13c5ba7c8a0f",
   "metadata": {},
   "source": [
    "### The Seq2Seq Model - Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aea489d-878d-42fa-89d7-aceb1a6fbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b57e11-b8eb-41af-8e9d-b81649ef3641",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec9351d-36aa-486b-a158-ab351a16b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed07cf-9faa-47e4-8bfc-8cbb40ef2a71",
   "metadata": {},
   "source": [
    "### Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd7c175-b3f6-4d10-8ee8-7fe2024e2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d504c95-206e-431c-b643-510efebb4d1c",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c49d795-fffc-43d8-a1ae-dfef1d30fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b97484-6ed0-4ea2-97a1-895cea426067",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb85dac-85f9-4aa7-b988-e00fa6affd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Encoder model\n",
    "encoder_load = torch.load('data/encoder_RNN.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8591c529-0611-4582-96be-232e267fd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Encoder model\n",
    "decoder_load = torch.load('data/decoder_RNN.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "072b98f6-cefd-4c97-bde6-f4febe74b0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoderRNN(\n",
       "  (embedding): Embedding(5161, 128)\n",
       "  (attention): BahdanauAttention(\n",
       "    (Wa): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (Ua): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (Va): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (gru): GRU(256, 128, batch_first=True)\n",
       "  (out): Linear(in_features=128, out_features=5161, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder_load.eval()\n",
    "decoder_load.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394c312-1117-40ec-8a18-addadf43224c",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79d8135f-c468-4db6-843c-3e23835c7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dafeaf6-e841-4812-a151-12be7c250e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 13056 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3378\n",
      "fra 5161\n"
     ]
    }
   ],
   "source": [
    "input_lang_2, output_lang_2, _ = get_dataloader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5959e941-1ba7-433b-8391-169b39819e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation is mostly the same as training, but there are no targets so we simply feed the decoder’s predictions back\n",
    "# to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token\n",
    "# we stop there. We also store the decoder’s attention outputs for display later.\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                #decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "556ed4e8-8d8f-454f-acf7-d3b6c71dd51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can evaluate random sentences from the training set and print out the input, target, and output \n",
    "# to make some subjective quality judgements\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bafa4c5-b2d7-4752-acb2-8c7ed3c137ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnInput(input_sentence):\n",
    "    output_words, _ = evaluate(encoder_load, decoder_load, normalizeString(input_sentence), input_lang_2, output_lang_2)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4fc91-2215-4f36-b905-53dd15ad2d29",
   "metadata": {},
   "source": [
    "### Run translation by randomly selecting 10 sentences from a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f573ab7c-027a-42be-a11f-cc362c017815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> you re out of control\n",
      "= tu es hors de controle\n",
      "< votre chance hors de controle\n",
      "\n",
      "> he is leaving chicago tomorrow\n",
      "= il va quitter chicago demain\n",
      "< il demain ses propres derniers elections demain\n",
      "\n",
      "> she is fit for the job\n",
      "= elle est apte pour le poste\n",
      "< elle du voyage au poste\n",
      "\n",
      "> you re unethical\n",
      "= tu es immoral\n",
      "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "> you re opportunistic\n",
      "= vous etes une opportuniste\n",
      "< vous etes\n",
      "\n",
      "> she s an intelligent young woman\n",
      "= c est une jeune fille intelligente\n",
      "< c est une femme blonde\n",
      "\n",
      "> she is an excellent student\n",
      "= c est une excellente etudiante\n",
      "< le voyage medecin\n",
      "\n",
      "> i m thrilled\n",
      "= je suis ravi\n",
      "< je suis ravi\n",
      "\n",
      "> we re engaged\n",
      "= nous sommes fiances\n",
      "< nous fiances\n",
      "\n",
      "> we are still clinging to the dreams of our youth\n",
      "= nous nous raccrochons encore aux reves de notre jeunesse\n",
      "< notre infirmiere encore un de rapide de notre jeunesse aient trop reves de notre jeunesse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder_load, decoder_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2dfa1-9774-4e1b-ad0e-025a72a8d8d7",
   "metadata": {},
   "source": [
    "### Translate an input sentence manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b547acd3-cafb-4ab2-a79d-0e9cc1736654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = I'm a salesman for our company!\n",
      "output = je suis representant de commerce pour notre societe !\n"
     ]
    }
   ],
   "source": [
    "evaluateOnInput(\"I'm a salesman for our company!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cfe6b78-c3a6-43dd-84a9-4ef34537a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = He's a construction worker\n",
      "output = n est un employe travailleur du batiment\n"
     ]
    }
   ],
   "source": [
    "evaluateOnInput(\"He's a construction worker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e241049c-0645-4b01-a26b-a00408e16605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = I'm going to take a bath\n",
      "output = ma veux me baigner\n"
     ]
    }
   ],
   "source": [
    "evaluateOnInput(\"I'm going to take a bath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "965f1df3-518d-43bf-9f98-e801ae956d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = No one is home\n",
      "output = pour n est pas une chez lui\n"
     ]
    }
   ],
   "source": [
    "evaluateOnInput(\"No one is home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1d214f5-9e6e-4fcd-8109-3197eeba2687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = No one's working\n",
      "output = pour n ai les personnes dans les autres arrivent\n"
     ]
    }
   ],
   "source": [
    "evaluateOnInput(\"No one's working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3bb259-47ba-4821-bb3e-36438154dc54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
